{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation of the Predicates Clustering Service using SciBERT Embeddings\n",
        "Using this notebook you can simply build either **K-Means** or **Agglomerative** clusters using pre-computed SciBERT embeddings of your dataset and then evaluate your trained models on your test set.\n",
        "\n",
        "Please store you data files in a Google Drive directory of yours(``MAIN_DRIVE_DIR``) and then provide their paths and the path (``RESULTS_PATH``) of where the results must be stored in that directory. Also please provide the name of your Google Storage Cloud bucket so that the models can be uploaded to and downloaded from it.\n",
        "\n",
        "\n",
        "|       Variable       | Description |\n",
        "|:--------------------:|:--------------------------------------------------------:|\n",
        "|``MAIN_DRIVE_DIR`` | Name of your main directory in your Google Drive |\n",
        "|  `TRAINING_SET_PATH` | Path to your training set inside the `MAIN_DRIVE_DIR `|\n",
        "|  `TEST_SET_PATH` | Path to your test set inside the `MAIN_DRIVE_DIR `|\n",
        "|  `DATA_PATH` | Path to your dataset inside the ``MAIN_DRIVE_DIR`` |\n",
        "|  `SCIBERT_TRAINING_REPRESENTATION` | Path to your training set representations the ``MAIN_DRIVE_DIR``. This should be the output file of the notebook ``SciBERT_embeddings``|\n",
        "|  `SCIBERT_TEST_REPRESENTATION` | Path to your test set representations the ``MAIN_DRIVE_DIR``. This should be the output file of the notebook ``SciBERT_embeddings``|\n",
        "|  `RESULTS_PATH` | Path to the generated results file inside the `MAIN_DRIVE_DIR `|\n",
        "|  `CLUSTERING_METHOD` | `kmeans` or `agglomerative`|\n",
        "| `VECTORIZER_PATH` | Name of the pickle file of your vectorizer |\n",
        "| `MODEL_TEMPLATE_PATH` | Template name of the model files which will be stored in your Google Cloud Storage. The default name has the schema `scibert_{clustering method}_{k}.pkl` |\n",
        "| `BUCKET` | The name of your Google Cloud Storage bucket |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eveKDY6nD8Y0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1qizpzlWjWg"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "TRAINING_SET_PATH = './training_set.json'\n",
        "TEST_SET_PATH = './test_set.json'\n",
        "DATA_PATH = './dataset.json'\n",
        "MAIN_DRIVE_DIR = 'TODO'\n",
        "SCIBERT_TRAINING_REPRESENTATION = './scibert_training_representations.npz'\n",
        "SCIBERT_TEST_REPRESENTATION = './scibert_test_representations.npz'\n",
        "VECTORIZER_PATH = './scibertvectorizer.pkl'\n",
        "CLUSTERING_METHOD = 'kmeans' # or 'agglomerative'\n",
        "RESULTS_PATH = './scibert_{}_results.json'.format(CLUSTERING_METHOD)\n",
        "BUCKET = 'TODO'\n",
        "MODEL_TEMPLATE_PATH = 'scibert_{}_{}.pkl'.format(CLUSTERING_METHOD, '{}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc6KZSPhU09O"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "def round_to_next_hundred(x):\n",
        "  return int(math.ceil(x / 100.0)) * 100\n",
        "\n",
        "\n",
        "def read_json(input_path):\n",
        "    with open(input_path, encoding='utf-8') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "def read_pickle(input_path):\n",
        "    with open(input_path, 'rb') as f:\n",
        "        loaded_object = pickle.load(f)\n",
        "    return loaded_object\n",
        "\n",
        "def write_json(json_data, output_path):\n",
        "    with open(output_path, 'w') as json_file:\n",
        "        json.dump(json_data, json_file, indent=4)\n",
        "\n",
        "def write_pickle(data, output_path):\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(data, f)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQoakl_pVpQn"
      },
      "outputs": [],
      "source": [
        "## Mount Drive into Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/'$TRAINING_SET_PATH $TRAINING_SET_PATH\n",
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/'$TEST_SET_PATH $TEST_SET_PATH\n",
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/'$DATA_PATH $DATA_PATH\n",
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/'$DATA_PATH $DATA_PATH\n",
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/scibert/'$SCIBERT_TRAINING_REPRESENTATION $SCIBERT_TRAINING_REPRESENTATION\n",
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/scibert/'$SCIBERT_TEST_REPRESENTATION $SCIBERT_TEST_REPRESENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ql4z5fwWzE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6t3EWXNXCX-"
      },
      "outputs": [],
      "source": [
        "# Import and process the training data\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "\n",
        "train_json = read_json(TRAINING_SET_PATH)\n",
        "train_df = pd.json_normalize(train_json['instances'])\n",
        "train_df_transformed = sparse.load_npz(SCIBERT_TRAINING_REPRESENTATION)\n",
        "\n",
        "n_comparisons = round_to_next_hundred(train_df.comparison_id.unique().shape[0])\n",
        "n_papers = round_to_next_hundred(train_df.paper_id.unique().shape[0])\n",
        "N_CLUSTERS = range(n_comparisons, n_papers, 50)\n",
        "print(list(N_CLUSTERS))\n",
        "\n",
        "# we need to build the clusters on the complete dataset, since the \"prediction\" in hierarchical clusterings requires re-building the clusters.\n",
        "if CLUSTERING_METHOD == 'agglomerative':\n",
        "  test_df_transformed = sparse.load_npz(SCIBERT_TEST_REPRESENTATION)\n",
        "  train_df_transformed = sparse.vstack((train_df_transformed, test_df_transformed))\n",
        "\n",
        "train_df_transformed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajMbCYNEcB_X"
      },
      "outputs": [],
      "source": [
        "#checking for optimal number of clusters\n",
        "from sklearn.cluster import KMeans \n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from time import time\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "  \n",
        "for n in N_CLUSTERS:\n",
        "    t0 = time()\n",
        "    MODEL_PATH = MODEL_TEMPLATE_PATH.format(n)\n",
        "    print(MODEL_PATH)\n",
        "\n",
        "    if CLUSTERING_METHOD == 'kmeans':\n",
        "      clustering_model = KMeans(n_clusters=n, random_state=212)\n",
        "      clustering_model = clustering_model.fit(train_df_transformed)\n",
        "    elif CLUSTERING_METHOD == 'agglomerative':\n",
        "      clustering_model = AgglomerativeClustering(n_clusters=n, linkage='ward')\n",
        "      clustering_model = clustering_model.fit(train_df_transformed.toarray())\n",
        "    \n",
        "    print('{0:2f}'.format(time() - t0))\n",
        "    write_pickle(clustering_model, MODEL_PATH)\n",
        "    \n",
        "    # Upload model to bucket\n",
        "    !gsutil cp {MODEL_PATH} gs://{BUCKET}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaFPVlNcwaag"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW7VcgMNHBPr"
      },
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dafHc0wZqtJ"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1QNuoRKEx2Z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ClusteringDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.data = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        instances = self.data['text']\n",
        "        \n",
        "        return '[CLS] ' + instances.iloc[[idx]].values[0] + ' [SEP]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGSjY1U1buim"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "vectorizer = SentenceTransformer('allenai/scibert_scivocab_uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54I0Vb5XHHVx"
      },
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xan3GnKHnm6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_comparisons(clustering_model, test_element, test_element_index, train_df):\n",
        "    if CLUSTERING_METHOD == 'kmeans':\n",
        "      cluster_label = clustering_model.predict(test_element)\n",
        "      cluster_instances_indices = np.argwhere(clustering_model.labels_ == cluster_label).squeeze(1)\n",
        "    elif CLUSTERING_METHOD == 'agglomerative':\n",
        "      cluster_label = clustering_model.labels_[train_df.shape[0] + test_element_index]\n",
        "      cluster_instances_indices = np.argwhere(clustering_model.labels_[:train_df.shape[0]] == cluster_label).squeeze(1)\n",
        "\n",
        "    cluster_instances = train_df.iloc[cluster_instances_indices]\n",
        "    comparison_ids = cluster_instances['comparison_id'].unique()\n",
        "    return comparison_ids\n",
        "\n",
        "\n",
        "def map_to_predicates(data, comparison_ids):\n",
        "    predicate_ids = []\n",
        "    \n",
        "    for comparison in data['comparisons']:\n",
        "      if comparison['id'] in comparison_ids:\n",
        "\n",
        "        for predicate in comparison['predicates']:\n",
        "          if predicate['id'] in predicate_ids:\n",
        "            continue\n",
        "\n",
        "          predicate_ids.append(predicate['id'])\n",
        "\n",
        "    return predicate_ids\n",
        "\n",
        "def evaluate_macro(expected, predicted):\n",
        "    return compute_metrics(evaluate_micro(expected, predicted))\n",
        "\n",
        "def evaluate_micro(expected, predicted):\n",
        "    \"\"\"\n",
        "    tp: correctly predicted properties --> found in expected and predicted sets\n",
        "    fp: incorrectly predicted properties --> found only in predicted set\n",
        "    fn: incorrectly predicted properties for other classes -> found only in expected set\n",
        "    \"\"\"\n",
        "    tp = len(set(expected).intersection(predicted))\n",
        "    fp = len(set(predicted).difference(expected))\n",
        "    fn = len(set(expected).difference(predicted))\n",
        "    \n",
        "    return np.array([tp, fp, fn])\n",
        "\n",
        "\n",
        "def compute_metrics(confusion_results):\n",
        "    tp, fp, fn = confusion_results\n",
        "    \n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f_measure = 2 * ((precision * recall) / (precision + recall)) \n",
        "\n",
        "    return np.array([precision, recall, f_measure])\n",
        "\n",
        "def f_measure(precision, recall):\n",
        "  return 2 * ((precision * recall) / (precision + recall))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH9luzNZxsaz"
      },
      "outputs": [],
      "source": [
        "# Import and process the test data\n",
        "import pandas as pd \n",
        "\n",
        "data = read_json(DATA_PATH)\n",
        "\n",
        "train_json = read_json(TRAINING_SET_PATH)\n",
        "train_df = pd.json_normalize(train_json['instances'])\n",
        "\n",
        "test_json = read_json(TEST_SET_PATH)\n",
        "test_df = pd.json_normalize(test_json['instances'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQuleMcqJqPl"
      },
      "source": [
        "## Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkbjPdC3Bv8h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from scipy import sparse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# vectorizer = read_pickle(VECTORIZER_PATH)\n",
        "\n",
        "results = {}\n",
        "vectorized_texts = np.empty((0, 768), dtype=np.float32)\n",
        "for i, k in enumerate([3150]):#N_CLUSTERS):\n",
        "    MODEL_PATH = MODEL_TEMPLATE_PATH.format(k)\n",
        "    print('evaluating model: {}'.format(MODEL_PATH))\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        !gsutil cp gs://$BUCKET/$MODEL_PATH $MODEL_PATH\n",
        "    clustering_model = read_pickle(MODEL_PATH)\n",
        "\n",
        "    macro_measures = np.empty((0,3), dtype=np.float32)\n",
        "    micro_measures = np.zeros(3)\n",
        "    for test_instance_index, test_instance in test_df.iterrows():\n",
        "        expected_comparison_id, text = test_instance['comparison_id'], test_instance['text']\n",
        "        expected = map_to_predicates(data, [expected_comparison_id])\n",
        "\n",
        "        # transform the texts only once. First iteration takes ~15 minutes\n",
        "        if i == 0:\n",
        "            vectorized_text = vectorizer.encode([text])\n",
        "            vectorized_texts = np.vstack((vectorized_texts, vectorized_text))\n",
        "        else:\n",
        "          vectorized_text = vectorized_texts[test_instance_index]\n",
        "\n",
        "        predicted_comparison_ids = predict_comparisons(clustering_model, vectorized_text.reshape(1, -1), test_instance_index, train_df)\n",
        "        predicted = map_to_predicates(data, predicted_comparison_ids)\n",
        "\n",
        "        macro_measures = np.vstack((macro_measures, evaluate_macro(expected, predicted)))\n",
        "        micro_measures += evaluate_micro(expected, predicted)\n",
        "\n",
        "    macro_measures = np.nanmean(macro_measures, axis=0)\n",
        "    micro_measures = compute_metrics(micro_measures)\n",
        "    results[str(k)] = {\n",
        "        'k': k,\n",
        "        'macro': {\n",
        "            'precision': macro_measures[0],\n",
        "            'recall': macro_measures[1],\n",
        "            'f_measure': f_measure(macro_measures[0], macro_measures[1])\n",
        "        },\n",
        "        'micro': {\n",
        "            'precision': micro_measures[0],\n",
        "            'recall': micro_measures[1],\n",
        "            'f_measure': micro_measures[2]\n",
        "        }\n",
        "    }\n",
        "    write_json(results, RESULTS_PATH)\n",
        "    !cp $RESULTS_PATH '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/scibert/'$RESULTS_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FhwHP3EV3-AH",
        "outputId": "6a047460-7b24-4ef5-bc51-e2bdb42fd150"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       k  macro.precision  macro.recall  macro.f_measure  micro.precision  \\\n",
              "0    400         0.306195      0.839694         0.448752         0.150197   \n",
              "1    450         0.305653      0.843269         0.448677         0.152694   \n",
              "2    500         0.320495      0.840201         0.463997         0.171163   \n",
              "3    550         0.331861      0.840069         0.475772         0.176141   \n",
              "4    600         0.335330      0.837539         0.478914         0.187117   \n",
              "5    650         0.352459      0.832684         0.495277         0.202559   \n",
              "6    700         0.371371      0.826300         0.512434         0.195432   \n",
              "7    750         0.370641      0.819199         0.510369         0.195210   \n",
              "8    800         0.369202      0.810297         0.507272         0.215505   \n",
              "9    850         0.402385      0.811629         0.538029         0.232339   \n",
              "10   900         0.391963      0.817461         0.529863         0.220137   \n",
              "11   950         0.404252      0.807717         0.538828         0.232702   \n",
              "12  1000         0.417650      0.814907         0.552260         0.253423   \n",
              "13  1050         0.418133      0.823888         0.554732         0.255884   \n",
              "14  1100         0.417264      0.796422         0.547618         0.267535   \n",
              "15  1150         0.423286      0.792098         0.551733         0.261860   \n",
              "16  1200         0.434911      0.789981         0.560983         0.274705   \n",
              "17  1250         0.432006      0.801976         0.561529         0.266838   \n",
              "18  1300         0.437258      0.790987         0.563187         0.260066   \n",
              "19  1350         0.446072      0.789872         0.570155         0.276768   \n",
              "20  1400         0.463300      0.802458         0.587440         0.253235   \n",
              "21  1450         0.447835      0.792180         0.572196         0.287987   \n",
              "22  1500         0.457199      0.785330         0.577938         0.296765   \n",
              "23  1550         0.456004      0.774343         0.573990         0.303542   \n",
              "24  1600         0.471483      0.781353         0.588097         0.298114   \n",
              "25  1650         0.465395      0.776452         0.581966         0.290133   \n",
              "26  1700         0.472825      0.754782         0.581423         0.317202   \n",
              "27  1750         0.473498      0.762629         0.584249         0.334079   \n",
              "28  1800         0.470799      0.766008         0.583173         0.316078   \n",
              "29  1850         0.482190      0.750913         0.587271         0.312531   \n",
              "30  1900         0.491723      0.767607         0.599445         0.318654   \n",
              "31  1950         0.503687      0.771094         0.609344         0.334388   \n",
              "32  2000         0.507553      0.755460         0.607177         0.351342   \n",
              "33  2050         0.494859      0.746409         0.595145         0.320553   \n",
              "34  2100         0.511072      0.746506         0.606751         0.352513   \n",
              "35  2150         0.509001      0.748620         0.605983         0.364797   \n",
              "36  2200         0.515471      0.750048         0.611019         0.355256   \n",
              "37  2250         0.523645      0.741636         0.613862         0.347787   \n",
              "38  2300         0.528106      0.738870         0.615957         0.355271   \n",
              "39  2350         0.539061      0.748878         0.626879         0.373098   \n",
              "40  2400         0.550363      0.737589         0.630368         0.385148   \n",
              "41  2450         0.545023      0.744481         0.629326         0.390165   \n",
              "42  2500         0.562574      0.738993         0.638827         0.405497   \n",
              "43  2550         0.562903      0.730034         0.635666         0.394166   \n",
              "44  2600         0.587501      0.735175         0.653094         0.407106   \n",
              "45  2650         0.591801      0.727164         0.652537         0.408996   \n",
              "46  2700         0.588648      0.732331         0.652676         0.444911   \n",
              "47  2750         0.599960      0.722872         0.655705         0.433826   \n",
              "48  2800         0.605350      0.721065         0.658160         0.439184   \n",
              "49  2850         0.616903      0.719458         0.664245         0.477697   \n",
              "50  2900         0.615004      0.717309         0.662228         0.449333   \n",
              "51  2950         0.625904      0.714605         0.667320         0.462797   \n",
              "52  3000         0.632347      0.707104         0.667639         0.469386   \n",
              "53  3050         0.641724      0.711316         0.674730         0.486274   \n",
              "54  3100         0.646721      0.714122         0.678752         0.499656   \n",
              "55  3150         0.654737      0.711628         0.681998         0.529390   \n",
              "\n",
              "    micro.recall  micro.f_measure  \n",
              "0       0.816065         0.253701  \n",
              "1       0.809686         0.256934  \n",
              "2       0.809835         0.282597  \n",
              "3       0.809167         0.289306  \n",
              "4       0.809241         0.303953  \n",
              "5       0.780687         0.321660  \n",
              "6       0.781132         0.312643  \n",
              "7       0.758733         0.310527  \n",
              "8       0.754580         0.335261  \n",
              "9       0.759846         0.355865  \n",
              "10      0.765557         0.341947  \n",
              "11      0.766002         0.356963  \n",
              "12      0.764592         0.380673  \n",
              "13      0.799154         0.387646  \n",
              "14      0.737225         0.392598  \n",
              "15      0.745902         0.387635  \n",
              "16      0.743455         0.401177  \n",
              "17      0.769265         0.396233  \n",
              "18      0.753096         0.386620  \n",
              "19      0.748646         0.404132  \n",
              "20      0.753393         0.379058  \n",
              "21      0.735593         0.413923  \n",
              "22      0.740933         0.423790  \n",
              "23      0.728473         0.428525  \n",
              "24      0.743306         0.425554  \n",
              "25      0.725580         0.414516  \n",
              "26      0.702959         0.437147  \n",
              "27      0.719499         0.456292  \n",
              "28      0.714604         0.438293  \n",
              "29      0.696803         0.431518  \n",
              "30      0.716977         0.441214  \n",
              "31      0.736705         0.459989  \n",
              "32      0.699844         0.467824  \n",
              "33      0.690128         0.437769  \n",
              "34      0.671141         0.462238  \n",
              "35      0.697100         0.478954  \n",
              "36      0.692279         0.469553  \n",
              "37      0.683527         0.461007  \n",
              "38      0.672625         0.464958  \n",
              "39      0.683824         0.482786  \n",
              "40      0.667804         0.488538  \n",
              "41      0.672625         0.493860  \n",
              "42      0.675146         0.506679  \n",
              "43      0.664466         0.494808  \n",
              "44      0.674776         0.507828  \n",
              "45      0.668991         0.507640  \n",
              "46      0.668175         0.534152  \n",
              "47      0.666617         0.525599  \n",
              "48      0.667359         0.529746  \n",
              "49      0.658459         0.553698  \n",
              "50      0.669584         0.537781  \n",
              "51      0.671216         0.547854  \n",
              "52      0.647037         0.544077  \n",
              "53      0.650300         0.556451  \n",
              "54      0.646518         0.563678  \n",
              "55      0.642587         0.580522  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51586b06-0b2c-46ca-b683-d9c5b5ab6e45\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>macro.precision</th>\n",
              "      <th>macro.recall</th>\n",
              "      <th>macro.f_measure</th>\n",
              "      <th>micro.precision</th>\n",
              "      <th>micro.recall</th>\n",
              "      <th>micro.f_measure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>400</td>\n",
              "      <td>0.306195</td>\n",
              "      <td>0.839694</td>\n",
              "      <td>0.448752</td>\n",
              "      <td>0.150197</td>\n",
              "      <td>0.816065</td>\n",
              "      <td>0.253701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>450</td>\n",
              "      <td>0.305653</td>\n",
              "      <td>0.843269</td>\n",
              "      <td>0.448677</td>\n",
              "      <td>0.152694</td>\n",
              "      <td>0.809686</td>\n",
              "      <td>0.256934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>500</td>\n",
              "      <td>0.320495</td>\n",
              "      <td>0.840201</td>\n",
              "      <td>0.463997</td>\n",
              "      <td>0.171163</td>\n",
              "      <td>0.809835</td>\n",
              "      <td>0.282597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>550</td>\n",
              "      <td>0.331861</td>\n",
              "      <td>0.840069</td>\n",
              "      <td>0.475772</td>\n",
              "      <td>0.176141</td>\n",
              "      <td>0.809167</td>\n",
              "      <td>0.289306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>600</td>\n",
              "      <td>0.335330</td>\n",
              "      <td>0.837539</td>\n",
              "      <td>0.478914</td>\n",
              "      <td>0.187117</td>\n",
              "      <td>0.809241</td>\n",
              "      <td>0.303953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>650</td>\n",
              "      <td>0.352459</td>\n",
              "      <td>0.832684</td>\n",
              "      <td>0.495277</td>\n",
              "      <td>0.202559</td>\n",
              "      <td>0.780687</td>\n",
              "      <td>0.321660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>700</td>\n",
              "      <td>0.371371</td>\n",
              "      <td>0.826300</td>\n",
              "      <td>0.512434</td>\n",
              "      <td>0.195432</td>\n",
              "      <td>0.781132</td>\n",
              "      <td>0.312643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>750</td>\n",
              "      <td>0.370641</td>\n",
              "      <td>0.819199</td>\n",
              "      <td>0.510369</td>\n",
              "      <td>0.195210</td>\n",
              "      <td>0.758733</td>\n",
              "      <td>0.310527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>800</td>\n",
              "      <td>0.369202</td>\n",
              "      <td>0.810297</td>\n",
              "      <td>0.507272</td>\n",
              "      <td>0.215505</td>\n",
              "      <td>0.754580</td>\n",
              "      <td>0.335261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>850</td>\n",
              "      <td>0.402385</td>\n",
              "      <td>0.811629</td>\n",
              "      <td>0.538029</td>\n",
              "      <td>0.232339</td>\n",
              "      <td>0.759846</td>\n",
              "      <td>0.355865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>900</td>\n",
              "      <td>0.391963</td>\n",
              "      <td>0.817461</td>\n",
              "      <td>0.529863</td>\n",
              "      <td>0.220137</td>\n",
              "      <td>0.765557</td>\n",
              "      <td>0.341947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>950</td>\n",
              "      <td>0.404252</td>\n",
              "      <td>0.807717</td>\n",
              "      <td>0.538828</td>\n",
              "      <td>0.232702</td>\n",
              "      <td>0.766002</td>\n",
              "      <td>0.356963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.417650</td>\n",
              "      <td>0.814907</td>\n",
              "      <td>0.552260</td>\n",
              "      <td>0.253423</td>\n",
              "      <td>0.764592</td>\n",
              "      <td>0.380673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1050</td>\n",
              "      <td>0.418133</td>\n",
              "      <td>0.823888</td>\n",
              "      <td>0.554732</td>\n",
              "      <td>0.255884</td>\n",
              "      <td>0.799154</td>\n",
              "      <td>0.387646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1100</td>\n",
              "      <td>0.417264</td>\n",
              "      <td>0.796422</td>\n",
              "      <td>0.547618</td>\n",
              "      <td>0.267535</td>\n",
              "      <td>0.737225</td>\n",
              "      <td>0.392598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1150</td>\n",
              "      <td>0.423286</td>\n",
              "      <td>0.792098</td>\n",
              "      <td>0.551733</td>\n",
              "      <td>0.261860</td>\n",
              "      <td>0.745902</td>\n",
              "      <td>0.387635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1200</td>\n",
              "      <td>0.434911</td>\n",
              "      <td>0.789981</td>\n",
              "      <td>0.560983</td>\n",
              "      <td>0.274705</td>\n",
              "      <td>0.743455</td>\n",
              "      <td>0.401177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1250</td>\n",
              "      <td>0.432006</td>\n",
              "      <td>0.801976</td>\n",
              "      <td>0.561529</td>\n",
              "      <td>0.266838</td>\n",
              "      <td>0.769265</td>\n",
              "      <td>0.396233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1300</td>\n",
              "      <td>0.437258</td>\n",
              "      <td>0.790987</td>\n",
              "      <td>0.563187</td>\n",
              "      <td>0.260066</td>\n",
              "      <td>0.753096</td>\n",
              "      <td>0.386620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1350</td>\n",
              "      <td>0.446072</td>\n",
              "      <td>0.789872</td>\n",
              "      <td>0.570155</td>\n",
              "      <td>0.276768</td>\n",
              "      <td>0.748646</td>\n",
              "      <td>0.404132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1400</td>\n",
              "      <td>0.463300</td>\n",
              "      <td>0.802458</td>\n",
              "      <td>0.587440</td>\n",
              "      <td>0.253235</td>\n",
              "      <td>0.753393</td>\n",
              "      <td>0.379058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1450</td>\n",
              "      <td>0.447835</td>\n",
              "      <td>0.792180</td>\n",
              "      <td>0.572196</td>\n",
              "      <td>0.287987</td>\n",
              "      <td>0.735593</td>\n",
              "      <td>0.413923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1500</td>\n",
              "      <td>0.457199</td>\n",
              "      <td>0.785330</td>\n",
              "      <td>0.577938</td>\n",
              "      <td>0.296765</td>\n",
              "      <td>0.740933</td>\n",
              "      <td>0.423790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1550</td>\n",
              "      <td>0.456004</td>\n",
              "      <td>0.774343</td>\n",
              "      <td>0.573990</td>\n",
              "      <td>0.303542</td>\n",
              "      <td>0.728473</td>\n",
              "      <td>0.428525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1600</td>\n",
              "      <td>0.471483</td>\n",
              "      <td>0.781353</td>\n",
              "      <td>0.588097</td>\n",
              "      <td>0.298114</td>\n",
              "      <td>0.743306</td>\n",
              "      <td>0.425554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1650</td>\n",
              "      <td>0.465395</td>\n",
              "      <td>0.776452</td>\n",
              "      <td>0.581966</td>\n",
              "      <td>0.290133</td>\n",
              "      <td>0.725580</td>\n",
              "      <td>0.414516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1700</td>\n",
              "      <td>0.472825</td>\n",
              "      <td>0.754782</td>\n",
              "      <td>0.581423</td>\n",
              "      <td>0.317202</td>\n",
              "      <td>0.702959</td>\n",
              "      <td>0.437147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1750</td>\n",
              "      <td>0.473498</td>\n",
              "      <td>0.762629</td>\n",
              "      <td>0.584249</td>\n",
              "      <td>0.334079</td>\n",
              "      <td>0.719499</td>\n",
              "      <td>0.456292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1800</td>\n",
              "      <td>0.470799</td>\n",
              "      <td>0.766008</td>\n",
              "      <td>0.583173</td>\n",
              "      <td>0.316078</td>\n",
              "      <td>0.714604</td>\n",
              "      <td>0.438293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1850</td>\n",
              "      <td>0.482190</td>\n",
              "      <td>0.750913</td>\n",
              "      <td>0.587271</td>\n",
              "      <td>0.312531</td>\n",
              "      <td>0.696803</td>\n",
              "      <td>0.431518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1900</td>\n",
              "      <td>0.491723</td>\n",
              "      <td>0.767607</td>\n",
              "      <td>0.599445</td>\n",
              "      <td>0.318654</td>\n",
              "      <td>0.716977</td>\n",
              "      <td>0.441214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1950</td>\n",
              "      <td>0.503687</td>\n",
              "      <td>0.771094</td>\n",
              "      <td>0.609344</td>\n",
              "      <td>0.334388</td>\n",
              "      <td>0.736705</td>\n",
              "      <td>0.459989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2000</td>\n",
              "      <td>0.507553</td>\n",
              "      <td>0.755460</td>\n",
              "      <td>0.607177</td>\n",
              "      <td>0.351342</td>\n",
              "      <td>0.699844</td>\n",
              "      <td>0.467824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2050</td>\n",
              "      <td>0.494859</td>\n",
              "      <td>0.746409</td>\n",
              "      <td>0.595145</td>\n",
              "      <td>0.320553</td>\n",
              "      <td>0.690128</td>\n",
              "      <td>0.437769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2100</td>\n",
              "      <td>0.511072</td>\n",
              "      <td>0.746506</td>\n",
              "      <td>0.606751</td>\n",
              "      <td>0.352513</td>\n",
              "      <td>0.671141</td>\n",
              "      <td>0.462238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>2150</td>\n",
              "      <td>0.509001</td>\n",
              "      <td>0.748620</td>\n",
              "      <td>0.605983</td>\n",
              "      <td>0.364797</td>\n",
              "      <td>0.697100</td>\n",
              "      <td>0.478954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2200</td>\n",
              "      <td>0.515471</td>\n",
              "      <td>0.750048</td>\n",
              "      <td>0.611019</td>\n",
              "      <td>0.355256</td>\n",
              "      <td>0.692279</td>\n",
              "      <td>0.469553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2250</td>\n",
              "      <td>0.523645</td>\n",
              "      <td>0.741636</td>\n",
              "      <td>0.613862</td>\n",
              "      <td>0.347787</td>\n",
              "      <td>0.683527</td>\n",
              "      <td>0.461007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2300</td>\n",
              "      <td>0.528106</td>\n",
              "      <td>0.738870</td>\n",
              "      <td>0.615957</td>\n",
              "      <td>0.355271</td>\n",
              "      <td>0.672625</td>\n",
              "      <td>0.464958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2350</td>\n",
              "      <td>0.539061</td>\n",
              "      <td>0.748878</td>\n",
              "      <td>0.626879</td>\n",
              "      <td>0.373098</td>\n",
              "      <td>0.683824</td>\n",
              "      <td>0.482786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2400</td>\n",
              "      <td>0.550363</td>\n",
              "      <td>0.737589</td>\n",
              "      <td>0.630368</td>\n",
              "      <td>0.385148</td>\n",
              "      <td>0.667804</td>\n",
              "      <td>0.488538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>2450</td>\n",
              "      <td>0.545023</td>\n",
              "      <td>0.744481</td>\n",
              "      <td>0.629326</td>\n",
              "      <td>0.390165</td>\n",
              "      <td>0.672625</td>\n",
              "      <td>0.493860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2500</td>\n",
              "      <td>0.562574</td>\n",
              "      <td>0.738993</td>\n",
              "      <td>0.638827</td>\n",
              "      <td>0.405497</td>\n",
              "      <td>0.675146</td>\n",
              "      <td>0.506679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>2550</td>\n",
              "      <td>0.562903</td>\n",
              "      <td>0.730034</td>\n",
              "      <td>0.635666</td>\n",
              "      <td>0.394166</td>\n",
              "      <td>0.664466</td>\n",
              "      <td>0.494808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>2600</td>\n",
              "      <td>0.587501</td>\n",
              "      <td>0.735175</td>\n",
              "      <td>0.653094</td>\n",
              "      <td>0.407106</td>\n",
              "      <td>0.674776</td>\n",
              "      <td>0.507828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>2650</td>\n",
              "      <td>0.591801</td>\n",
              "      <td>0.727164</td>\n",
              "      <td>0.652537</td>\n",
              "      <td>0.408996</td>\n",
              "      <td>0.668991</td>\n",
              "      <td>0.507640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>2700</td>\n",
              "      <td>0.588648</td>\n",
              "      <td>0.732331</td>\n",
              "      <td>0.652676</td>\n",
              "      <td>0.444911</td>\n",
              "      <td>0.668175</td>\n",
              "      <td>0.534152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>2750</td>\n",
              "      <td>0.599960</td>\n",
              "      <td>0.722872</td>\n",
              "      <td>0.655705</td>\n",
              "      <td>0.433826</td>\n",
              "      <td>0.666617</td>\n",
              "      <td>0.525599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>2800</td>\n",
              "      <td>0.605350</td>\n",
              "      <td>0.721065</td>\n",
              "      <td>0.658160</td>\n",
              "      <td>0.439184</td>\n",
              "      <td>0.667359</td>\n",
              "      <td>0.529746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>2850</td>\n",
              "      <td>0.616903</td>\n",
              "      <td>0.719458</td>\n",
              "      <td>0.664245</td>\n",
              "      <td>0.477697</td>\n",
              "      <td>0.658459</td>\n",
              "      <td>0.553698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>2900</td>\n",
              "      <td>0.615004</td>\n",
              "      <td>0.717309</td>\n",
              "      <td>0.662228</td>\n",
              "      <td>0.449333</td>\n",
              "      <td>0.669584</td>\n",
              "      <td>0.537781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>2950</td>\n",
              "      <td>0.625904</td>\n",
              "      <td>0.714605</td>\n",
              "      <td>0.667320</td>\n",
              "      <td>0.462797</td>\n",
              "      <td>0.671216</td>\n",
              "      <td>0.547854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>3000</td>\n",
              "      <td>0.632347</td>\n",
              "      <td>0.707104</td>\n",
              "      <td>0.667639</td>\n",
              "      <td>0.469386</td>\n",
              "      <td>0.647037</td>\n",
              "      <td>0.544077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>3050</td>\n",
              "      <td>0.641724</td>\n",
              "      <td>0.711316</td>\n",
              "      <td>0.674730</td>\n",
              "      <td>0.486274</td>\n",
              "      <td>0.650300</td>\n",
              "      <td>0.556451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>3100</td>\n",
              "      <td>0.646721</td>\n",
              "      <td>0.714122</td>\n",
              "      <td>0.678752</td>\n",
              "      <td>0.499656</td>\n",
              "      <td>0.646518</td>\n",
              "      <td>0.563678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>3150</td>\n",
              "      <td>0.654737</td>\n",
              "      <td>0.711628</td>\n",
              "      <td>0.681998</td>\n",
              "      <td>0.529390</td>\n",
              "      <td>0.642587</td>\n",
              "      <td>0.580522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51586b06-0b2c-46ca-b683-d9c5b5ab6e45')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51586b06-0b2c-46ca-b683-d9c5b5ab6e45 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51586b06-0b2c-46ca-b683-d9c5b5ab6e45');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/scibert/'$RESULTS_PATH $RESULTS_PATH\n",
        "\n",
        "results = read_json(RESULTS_PATH)\n",
        "results_df = pd.json_normalize(results.values())\n",
        "results_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}