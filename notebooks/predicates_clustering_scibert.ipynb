{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1qizpzlWjWg"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "TRAINING_SET_PATH = './training_set.json'\n",
        "TEST_SET_PATH = './test_set.json'\n",
        "DATA_PATH = './data.json'\n",
        "SCIBERT_TRAINING_REPRESENTATION = './scibert_training_representations.npz'\n",
        "SCIBERT_TEST_REPRESENTATION = './scibert_test_representations.npz'\n",
        "VECTORIZER_PATH = './scibertvectorizer.pkl'\n",
        "CLUSTERING_METHOD = 'agglomerative' # choices ['kmeans', 'agglomerative']\n",
        "RESULTS_PATH = './scibert_{}_results.json'.format(CLUSTERING_METHOD)\n",
        "BUCKET = 'YOUR_BUCKET_ID'\n",
        "MODEL_TEMPLATE_PATH = 'scibert_{}_{}.pkl'.format(CLUSTERING_METHOD, '{}')\n",
        "BERT_PATH = './scibert_scivocab_uncased'\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "N_CLUSTERS = range(200, 2100, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc6KZSPhU09O"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "\n",
        "def read_json(input_path):\n",
        "    with open(input_path, encoding='utf-8') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "def read_pickle(input_path):\n",
        "    with open(input_path, 'rb') as f:\n",
        "        loaded_object = pickle.load(f)\n",
        "    return loaded_object\n",
        "\n",
        "def write_json(json_data, output_path):\n",
        "    with open(output_path, 'w') as json_file:\n",
        "        json.dump(json_data, json_file, indent=4)\n",
        "\n",
        "def write_pickle(data, output_path):\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(data, f)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQoakl_pVpQn",
        "outputId": "e4eb69c6-4b5f-44ba-b335-8e7d3b9826ae"
      },
      "outputs": [],
      "source": [
        "## Mount Drive into Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp '/path_to_your_directory_on_google_drive/training_set.json' $TRAINING_SET_PATH\n",
        "!cp '/path_to_your_directory_on_google_drive/test_set.json' $TEST_SET_PATH\n",
        "!cp '/path_to_your_directory_on_google_drive/data.json' $DATA_PATH\n",
        "!cp '/path_to_your_directory_on_google_drive/'$SCIBERT_TRAINING_REPRESENTATION $SCIBERT_TRAINING_REPRESENTATION\n",
        "!cp '/path_to_your_directory_on_google_drive/'$SCIBERT_TEST_REPRESENTATION $SCIBERT_TEST_REPRESENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ql4z5fwWzE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6t3EWXNXCX-",
        "outputId": "729b2856-7c78-4d19-8b11-e373e2c877d0"
      },
      "outputs": [],
      "source": [
        "# Import and process the training data\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "\n",
        "train_json = read_json(TRAINING_SET_PATH)\n",
        "train_df = pd.json_normalize(train_json['instances'])\n",
        "train_df_transformed = sparse.load_npz(SCIBERT_TRAINING_REPRESENTATION)\n",
        "\n",
        "# we need to build the clusters on the complete dataset, since the \"prediction\" in hierarchical clusterings requires re-building the clusters.\n",
        "if CLUSTERING_METHOD == 'agglomerative':\n",
        "  test_df_transformed = sparse.load_npz(SCIBERT_TEST_REPRESENTATION)\n",
        "  train_df_transformed = sparse.vstack((train_df_transformed, test_df_transformed))\n",
        "\n",
        "train_df_transformed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajMbCYNEcB_X"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans \n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from time import time\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "for n in N_CLUSTERS:\n",
        "    t0 = time()\n",
        "    MODEL_PATH = MODEL_TEMPLATE_PATH.format(n)\n",
        "    print(MODEL_PATH)\n",
        "\n",
        "    if CLUSTERING_METHOD == 'kmeans':\n",
        "      clustering_model = KMeans(n_clusters=n, random_state=212)\n",
        "      clustering_model = clustering_model.fit(train_df_transformed)\n",
        "    elif CLUSTERING_METHOD == 'agglomerative':\n",
        "      clustering_model = AgglomerativeClustering(n_clusters=n, linkage='ward')\n",
        "      clustering_model = clustering_model.fit(train_df_transformed.toarray())\n",
        "    \n",
        "    print('{0:2f}'.format(time() - t0))\n",
        "    write_pickle(clustering_model, MODEL_PATH)\n",
        "    \n",
        "    # Upload model to bucket\n",
        "    !gsutil cp {MODEL_PATH} gs://{BUCKET}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaFPVlNcwaag"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW7VcgMNHBPr"
      },
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44vLrUOtEnj9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "# Downloading the scibert model\n",
        "!wget -qO- https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar | tar --transform 's/^dbt2-0.37.50.3/dbt2/' -xv\n",
        "!tar -xzf ./scibert_scivocab_uncased/weights.tar.gz -C ./scibert_scivocab_uncased/\n",
        "!mv ./scibert_scivocab_uncased/bert_config.json ./scibert_scivocab_uncased/config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1QNuoRKEx2Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ClusteringDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.data = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        instances = self.data['text']\n",
        "        \n",
        "        return instances.iloc[[idx]].values[0]\n",
        "\n",
        "class SciBERT:\n",
        "    DEFAULT_PATH = 'allenai/scibert_scivocab_uncased'\n",
        "\n",
        "    @staticmethod\n",
        "    def model(path=DEFAULT_PATH):\n",
        "        return BertModel.from_pretrained(path)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(path=DEFAULT_PATH):\n",
        "        return BertTokenizer.from_pretrained(path)\n",
        "\n",
        "class SciBERTVectorizer:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, tokenizer, model):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, text_batch):\n",
        "    text_tokenized = self.tokenizer(text=text_batch, padding='max_length', max_length=MAX_SEQUENCE_LENGTH, truncation=True, return_tensors='pt')\n",
        "    outputs = self.model(text_tokenized['input_ids'])\n",
        "\n",
        "    return outputs['last_hidden_state'].squeeze(0).mean(0).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGSjY1U1buim"
      },
      "outputs": [],
      "source": [
        "tokenizer = SciBERT.tokenizer(BERT_PATH)\n",
        "model = SciBERT.model(BERT_PATH)\n",
        "\n",
        "vectorizer = SciBERTVectorizer().fit(tokenizer, model)\n",
        "write_pickle(vectorizer, VECTORIZER_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54I0Vb5XHHVx"
      },
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xan3GnKHnm6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_comparisons(clustering_model, test_element, test_element_index, train_df):\n",
        "    if CLUSTERING_METHOD == 'kmeans':\n",
        "      cluster_label = clustering_model.predict(test_element)\n",
        "      cluster_instances_indices = np.argwhere(clustering_model.labels_ == cluster_label).squeeze(1)\n",
        "    elif CLUSTERING_METHOD == 'agglomerative':\n",
        "      cluster_label = clustering_model.labels_[train_df.shape[0] + test_element_index]\n",
        "      cluster_instances_indices = np.argwhere(clustering_model.labels_[:train_df.shape[0]] == cluster_label).squeeze(1)\n",
        "\n",
        "    cluster_instances = train_df.iloc[cluster_instances_indices]\n",
        "    comparison_ids = cluster_instances['comparison_id'].unique()\n",
        "    return comparison_ids\n",
        "\n",
        "\n",
        "def map_to_predicates(data, comparison_ids):\n",
        "    predicate_ids = []\n",
        "    \n",
        "    for comparison in data['comparisons']:\n",
        "      if comparison['id'] in comparison_ids:\n",
        "\n",
        "        for predicate in comparison['predicates']:\n",
        "          if predicate['id'] in predicate_ids:\n",
        "            continue\n",
        "\n",
        "          predicate_ids.append(predicate['id'])\n",
        "\n",
        "    return predicate_ids\n",
        "\n",
        "def evaluate_macro(expected, predicted):\n",
        "    return compute_metrics(evaluate_micro(expected, predicted))\n",
        "\n",
        "def evaluate_micro(expected, predicted):\n",
        "    \"\"\"\n",
        "    tp: correctly predicted properties --> found in expected and predicted sets\n",
        "    fp: incorrectly predicted properties --> found only in predicted set\n",
        "    fn: incorrectly predicted properties for other classes -> found only in expected set\n",
        "    \"\"\"\n",
        "    tp = len(set(expected).intersection(predicted))\n",
        "    fp = len(set(predicted).difference(expected))\n",
        "    fn = len(set(expected).difference(predicted))\n",
        "    \n",
        "    return np.array([tp, fp, fn])\n",
        "\n",
        "\n",
        "def compute_metrics(confusion_results):\n",
        "    tp, fp, fn = confusion_results\n",
        "    \n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f_measure = 2 * ((precision * recall) / (precision + recall)) \n",
        "\n",
        "    return np.array([precision, recall, f_measure])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH9luzNZxsaz",
        "outputId": "891c7f61-7508-486a-f36c-d236e78929b8"
      },
      "outputs": [],
      "source": [
        "# Import and process the test data\n",
        "import pandas as pd \n",
        "\n",
        "data = read_json(DATA_PATH)\n",
        "\n",
        "train_json = read_json(TRAINING_SET_PATH)\n",
        "train_df = pd.json_normalize(train_json['instances'])\n",
        "\n",
        "test_json = read_json(TEST_SET_PATH)\n",
        "test_df = pd.json_normalize(test_json['instances'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQuleMcqJqPl"
      },
      "source": [
        "## Evalation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkbjPdC3Bv8h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from scipy import sparse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "vectorizer = read_pickle(VECTORIZER_PATH)\n",
        "\n",
        "results = {}\n",
        "vectorized_texts = np.empty((0, 768), dtype=np.float32)\n",
        "for i, k in enumerate(N_CLUSTERS):\n",
        "    MODEL_PATH = MODEL_TEMPLATE_PATH.format(k)\n",
        "    print('evaluating model: {}'.format(MODEL_PATH))\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        !gsutil cp gs://$BUCKET/$MODEL_PATH $MODEL_PATH\n",
        "    clustering_model = read_pickle(MODEL_PATH)\n",
        "\n",
        "    macro_measures = np.empty((0,3), dtype=np.float32)\n",
        "    micro_measures = np.zeros(3)\n",
        "    for test_instance_index, test_instance in test_df.iterrows():\n",
        "        expected_comparison_id, text = test_instance['comparison_id'], test_instance['text']\n",
        "        expected = map_to_predicates(data, [expected_comparison_id])\n",
        "\n",
        "        # transform the texts only once. First iteration takes ~15 minutes\n",
        "        if i == 0:\n",
        "            vectorized_text = vectorizer.transform([text])\n",
        "            vectorized_texts = np.vstack((vectorized_texts, vectorized_text))\n",
        "        else:\n",
        "          vectorized_text = vectorized_texts[test_instance_index]\n",
        "\n",
        "        predicted_comparison_ids = predict_comparisons(clustering_model, vectorized_text.reshape(1, -1), test_instance_index, train_df)\n",
        "        predicted = map_to_predicates(data, predicted_comparison_ids)\n",
        "        macro_measures = np.vstack((macro_measures, evaluate_macro(expected, predicted)))\n",
        "        micro_measures += evaluate_micro(expected, predicted)\n",
        "\n",
        "    macro_measures = np.nanmean(macro_measures, axis=0)\n",
        "    micro_measures = compute_metrics(micro_measures)\n",
        "    results[str(k)] = {\n",
        "        'k': k,\n",
        "        'macro': {\n",
        "            'precision': macro_measures[0],\n",
        "            'recall': macro_measures[1],\n",
        "            'f_measure': macro_measures[2]\n",
        "        },\n",
        "        'micro': {\n",
        "            'precision': micro_measures[0],\n",
        "            'recall': micro_measures[1],\n",
        "            'f_measure': micro_measures[2]\n",
        "        }\n",
        "    }\n",
        "    write_json(results, RESULTS_PATH)\n",
        "    !cp $RESULTS_PATH '/path_to_your_directory_on_google_drive/'$RESULTS_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FhwHP3EV3-AH",
        "outputId": "05417e7f-12a9-4329-8736-6b6dc7dfe31f"
      },
      "outputs": [],
      "source": [
        "!cp '/path_to_your_directory_on_google_drive/'$RESULTS_PATH $RESULTS_PATH\n",
        "\n",
        "results = read_json(RESULTS_PATH)\n",
        "results_df = pd.json_normalize(results.values())\n",
        "results_df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "predicates_clustering_scibert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
