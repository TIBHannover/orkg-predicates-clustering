{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation of the Predicates Clustering Service using TF-IDF Embeddings\n",
        "Using this notebook you can simply build either **K-Means** or **Agglomerative** clusters using TF-IDF embeddings of your dataset and then evaluate your trained models on your test set.\n",
        "\n",
        "Please store you data files in a Google Drive directory of yours(``MAIN_DRIVE_DIR``) and then provide their paths and the path (``RESULTS_PATH``) of where the results must be stored in that directory. Also please provide the name of your Google Storage Cloud bucket so that the models can be uploaded to and downloaded from it.\n",
        "\n",
        "\n",
        "|       Variable       | Description |\n",
        "|:--------------------:|:--------------------------------------------------------:|\n",
        "|``MAIN_DRIVE_DIR`` | Name of your main directory in your Google Drive |\n",
        "|  `TRAINING_SET_PATH` | Path to your training set inside the `MAIN_DRIVE_DIR `|\n",
        "|  `TEST_SET_PATH` | Path to your test set inside the `MAIN_DRIVE_DIR `|\n",
        "|  `DATA_PATH` | Path to your dataset inside the ``MAIN_DRIVE_DIR`` |\n",
        "|  `RESULTS_PATH` | Path to the generated results file inside the `MAIN_DRIVE_DIR `|\n",
        "|  `CLUSTERING_METHOD` | `kmeans` or `agglomerative`|\n",
        "| `VECTORIZER_PATH` | Name of the pickle file of your vectorizer |\n",
        "| `MODEL_TEMPLATE_PATH` | Template name of the model files which will be stored in your Google Cloud Storage. The default name has the schema `tfidf_{clustering method}_{k}.pkl` |\n",
        "| `BUCKET` | The name of your Google Cloud Storage bucket |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y_ifchy0K8cG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1qizpzlWjWg"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "TRAINING_SET_PATH = './training_set.json'\n",
        "TEST_SET_PATH = './test_set.json'\n",
        "DATA_PATH = './dataset.json'\n",
        "VECTORIZER_PATH = './tfidfvectorizer.pkl'\n",
        "MAIN_DRIVE_DIR = 'TODO'\n",
        "CLUSTERING_METHOD = 'kmeans' # or 'agglomerative'\n",
        "RESULTS_PATH = './tfidf_{}_results.json'.format(CLUSTERING_METHOD)\n",
        "EXPECTED_PREDICTED_PATH = './tfidf_{}_expected_predicted.json'.format(CLUSTERING_METHOD)\n",
        "BUCKET = 'TODO'\n",
        "MODEL_TEMPLATE_PATH = 'tfidf_{}_{}.pkl'.format(CLUSTERING_METHOD, '{}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iExIO2cvUhHB"
      },
      "outputs": [],
      "source": [
        "\"\"\"removes punctuation, stopwords, and returns a list of the remaining words, or tokens\"\"\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc6KZSPhU09O"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import json\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "def round_to_next_hundred(x):\n",
        "  return int(math.ceil(x / 100.0)) * 100\n",
        "\n",
        "def text_process(text):\n",
        "    '''\n",
        "    Takes in a string of text, then performs the following:\n",
        "    1. Remove all punctuation\n",
        "    2. Remove all stopwords\n",
        "    3. Return the cleaned text as a list of words\n",
        "    4. Remove words\n",
        "    '''\n",
        "    stemmer = WordNetLemmatizer()\n",
        "    nopunc = [char for char in text if char not in string.punctuation]\n",
        "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
        "    nopunc =  [word.lower() for word in nopunc.split() if word not in stopwords.words('english')]\n",
        "    return ' '.join([stemmer.lemmatize(word) for word in nopunc])\n",
        "\n",
        "def read_json(input_path):\n",
        "    with open(input_path, encoding='utf-8') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "def read_pickle(input_path):\n",
        "    with open(input_path, 'rb') as f:\n",
        "        loaded_object = pickle.load(f)\n",
        "    return loaded_object\n",
        "\n",
        "def write_json(json_data, output_path):\n",
        "    with open(output_path, 'w') as json_file:\n",
        "        json.dump(json_data, json_file, indent=4)\n",
        "\n",
        "def write_pickle(data, output_path):\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(data, f)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQoakl_pVpQn"
      },
      "outputs": [],
      "source": [
        "## Mount Drive into Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/'$TRAINING_SET_PATH $TRAINING_SET_PATH\n",
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/'$TEST_SET_PATH $TEST_SET_PATH\n",
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/'$DATA_PATH $DATA_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG_CKON1nQVd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6t3EWXNXCX-"
      },
      "outputs": [],
      "source": [
        "# Import and process the training data\n",
        "import pandas as pd\n",
        "\n",
        "train_json = read_json(TRAINING_SET_PATH)\n",
        "train_df = pd.json_normalize(train_json['instances'])\n",
        "train_df['text'] = train_df['text'].apply(text_process)\n",
        "\n",
        "n_comparisons = round_to_next_hundred(train_df.comparison_id.unique().shape[0])\n",
        "n_papers = round_to_next_hundred(train_df.paper_id.unique().shape[0])\n",
        "N_CLUSTERS = range(n_comparisons, n_papers, 50)\n",
        "print(list(N_CLUSTERS))\n",
        "\n",
        "\n",
        "if CLUSTERING_METHOD == 'agglomerative':\n",
        "  test_json = read_json(TEST_SET_PATH)\n",
        "  test_df = pd.json_normalize(test_json['instances'])\n",
        "  test_df['text'] = test_df['text'].apply(text_process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSjBnpPYalvQ"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2)).fit(train_df['text'])\n",
        "write_pickle(vectorizer, VECTORIZER_PATH)\n",
        "\n",
        "train_df_transformed = vectorizer.transform(train_df['text'])\n",
        "\n",
        "# we need to build the clusters on the complete dataset, since the \"prediction\" in hierarchical clusterings requires re-building the clusters.\n",
        "if CLUSTERING_METHOD == 'agglomerative':\n",
        "  test_df_transformed = vectorizer.transform(test_df['text'])\n",
        "  train_df_transformed = sparse.vstack((train_df_transformed, test_df_transformed))\n",
        "\n",
        "train_df_transformed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajMbCYNEcB_X"
      },
      "outputs": [],
      "source": [
        "#checking for optimal number of clusters\n",
        "from sklearn.cluster import KMeans \n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from time import time\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "for n in N_CLUSTERS:\n",
        "    t0 = time()\n",
        "    MODEL_PATH = MODEL_TEMPLATE_PATH.format(n)\n",
        "    print(MODEL_PATH)\n",
        "\n",
        "    if CLUSTERING_METHOD == 'kmeans':\n",
        "      clustering_model = KMeans(n_clusters=n, random_state=212)\n",
        "      clustering_model = clustering_model.fit(train_df_transformed)\n",
        "    elif CLUSTERING_METHOD == 'agglomerative':\n",
        "      clustering_model = AgglomerativeClustering(n_clusters=n, linkage='ward')\n",
        "      clustering_model = clustering_model.fit(train_df_transformed.toarray())\n",
        "\n",
        "    print('{0:2f}'.format(time() - t0))\n",
        "    write_pickle(clustering_model, MODEL_PATH)\n",
        "\n",
        "    # Upload model to bucket\n",
        "    !gsutil cp {MODEL_PATH} gs://{BUCKET}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFlX1rQznFRn"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xan3GnKHnm6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_comparisons(clustering_model, test_element, test_element_index, train_df):\n",
        "    if CLUSTERING_METHOD == 'kmeans':\n",
        "      cluster_label = clustering_model.predict(test_element)\n",
        "      cluster_instances_indices = np.argwhere(clustering_model.labels_ == cluster_label).squeeze(1)\n",
        "    elif CLUSTERING_METHOD == 'agglomerative':\n",
        "      cluster_label = clustering_model.labels_[train_df.shape[0] + test_element_index]\n",
        "      cluster_instances_indices = np.argwhere(clustering_model.labels_[:train_df.shape[0]] == cluster_label).squeeze(1)\n",
        "\n",
        "    cluster_instances = train_df.iloc[cluster_instances_indices]\n",
        "    comparison_ids = cluster_instances['comparison_id'].unique()\n",
        "    return comparison_ids\n",
        "\n",
        "\n",
        "def map_to_predicates(data, comparison_ids):\n",
        "    predicate_ids = []\n",
        "    \n",
        "    for comparison in data['comparisons']:\n",
        "      if comparison['id'] in comparison_ids:\n",
        "\n",
        "        for predicate in comparison['predicates']:\n",
        "          if predicate['id'] in predicate_ids:\n",
        "            continue\n",
        "\n",
        "          predicate_ids.append(predicate['id'])\n",
        "\n",
        "    return predicate_ids\n",
        "\n",
        "def evaluate_macro(expected, predicted):\n",
        "    return compute_metrics(evaluate_micro(expected, predicted))\n",
        "\n",
        "def evaluate_micro(expected, predicted):\n",
        "    \"\"\"\n",
        "    tp: correctly predicted properties --> found in expected and predicted sets\n",
        "    fp: incorrectly predicted properties --> found only in predicted set\n",
        "    fn: incorrectly predicted properties for other classes -> found only in expected set\n",
        "    \"\"\"\n",
        "    tp = len(set(expected).intersection(predicted))\n",
        "    fp = len(set(predicted).difference(expected))\n",
        "    fn = len(set(expected).difference(predicted))\n",
        "    \n",
        "    return np.array([tp, fp, fn])\n",
        "\n",
        "\n",
        "def compute_metrics(confusion_results):\n",
        "    tp, fp, fn = confusion_results\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f_measure = 2 * ((precision * recall) / (precision + recall)) \n",
        "    \n",
        "    return np.array([precision, recall, f_measure])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYNMih-knLLn"
      },
      "outputs": [],
      "source": [
        "# Import and process the test data\n",
        "import pandas as pd\n",
        "\n",
        "train_json = read_json(TRAINING_SET_PATH)\n",
        "train_df = pd.json_normalize(train_json['instances'])\n",
        "\n",
        "test_json = read_json(TEST_SET_PATH)\n",
        "test_df = pd.json_normalize(test_json['instances'])\n",
        "test_df['text'] = test_df['text'].apply(text_process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkbjPdC3Bv8h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "data = read_json(DATA_PATH)\n",
        "vectorizer = read_pickle(VECTORIZER_PATH)\n",
        "\n",
        "results = {}\n",
        "for n in N_CLUSTERS:\n",
        "    MODEL_PATH = MODEL_TEMPLATE_PATH.format(n)\n",
        "    print('evaluating model: {}'.format(MODEL_PATH))\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        !gsutil cp gs://$BUCKET/$MODEL_PATH $MODEL_PATH\n",
        "\n",
        "    try:    \n",
        "      clustering_model = read_pickle(MODEL_PATH)\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "    macro_measures = np.empty((0,3), float)\n",
        "    micro_measures = np.zeros(3)\n",
        "    for test_instance_index, test_instance in test_df.iterrows():\n",
        "        expected_comparison_id, text = test_instance['comparison_id'], test_instance['text']\n",
        "        expected = map_to_predicates(data, [expected_comparison_id])\n",
        "        vectorized_text = vectorizer.transform([text])\n",
        "\n",
        "        predicted_comparison_ids = predict_comparisons(clustering_model, vectorized_text, test_instance_index, train_df)\n",
        "        predicted = map_to_predicates(data, predicted_comparison_ids)\n",
        "        macro_measures = np.vstack((macro_measures, evaluate_macro(expected, predicted)))\n",
        "        micro_measures += evaluate_micro(expected, predicted)\n",
        "    \n",
        "    macro_measures = np.nanmean(macro_measures, axis=0)\n",
        "    micro_measures = compute_metrics(micro_measures)\n",
        "    results[str(n)] = {\n",
        "        'k': n,\n",
        "        'macro': {\n",
        "            'precision': macro_measures[0],\n",
        "            'recall': macro_measures[1],\n",
        "            'f_measure': macro_measures[2]\n",
        "        },\n",
        "        'micro': {\n",
        "            'precision': micro_measures[0],\n",
        "            'recall': micro_measures[1],\n",
        "            'f_measure': micro_measures[2]\n",
        "        }\n",
        "    }\n",
        "    write_json(results, RESULTS_PATH)\n",
        "    !cp $RESULTS_PATH '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/tfidf/'$RESULTS_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08hoZkOZr6_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "bee6815d-5574-4972-850f-314a03ea93e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       k  macro.precision  macro.recall  macro.f_measure  micro.precision  \\\n",
              "0    400         0.269173      0.845144         0.481503         0.062049   \n",
              "1    450         0.262240      0.807123         0.467562         0.046184   \n",
              "2    500         0.274906      0.804626         0.486940         0.084478   \n",
              "3    550         0.287113      0.796953         0.500554         0.069442   \n",
              "4    600         0.278936      0.773071         0.480268         0.056120   \n",
              "5    650         0.288118      0.765863         0.494907         0.051145   \n",
              "6    700         0.334216      0.788092         0.550270         0.089523   \n",
              "7    750         0.298055      0.766716         0.504257         0.050133   \n",
              "8    800         0.319702      0.755123         0.536700         0.077119   \n",
              "9    850         0.319544      0.741956         0.531180         0.072618   \n",
              "10   900         0.335903      0.775659         0.557400         0.084089   \n",
              "11   950         0.328482      0.738708         0.540658         0.088039   \n",
              "12  1000         0.334144      0.759744         0.550131         0.088152   \n",
              "13  1050         0.344332      0.759436         0.560776         0.093511   \n",
              "14  1100         0.391950      0.793246         0.624942         0.235231   \n",
              "15  1150         0.358814      0.773689         0.589179         0.099056   \n",
              "16  1200         0.361930      0.758660         0.586275         0.133203   \n",
              "17  1250         0.380419      0.759841         0.615940         0.246951   \n",
              "18  1300         0.362470      0.774552         0.595434         0.167399   \n",
              "\n",
              "    micro.recall  micro.f_measure  \n",
              "0       0.823111         0.115399  \n",
              "1       0.773715         0.087165  \n",
              "2       0.776533         0.152378  \n",
              "3       0.759030         0.127243  \n",
              "4       0.734332         0.104272  \n",
              "5       0.751613         0.095773  \n",
              "6       0.742416         0.159779  \n",
              "7       0.744419         0.093940  \n",
              "8       0.726322         0.139433  \n",
              "9       0.705184         0.131677  \n",
              "10      0.714159         0.150461  \n",
              "11      0.713788         0.156745  \n",
              "12      0.698064         0.156537  \n",
              "13      0.704072         0.165096  \n",
              "14      0.731514         0.355988  \n",
              "15      0.727880         0.174381  \n",
              "16      0.693688         0.223491  \n",
              "17      0.719424         0.367689  \n",
              "18      0.720463         0.271675  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-debb4e26-2995-4f27-9ec9-7f54fa5eec78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>macro.precision</th>\n",
              "      <th>macro.recall</th>\n",
              "      <th>macro.f_measure</th>\n",
              "      <th>micro.precision</th>\n",
              "      <th>micro.recall</th>\n",
              "      <th>micro.f_measure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>400</td>\n",
              "      <td>0.269173</td>\n",
              "      <td>0.845144</td>\n",
              "      <td>0.481503</td>\n",
              "      <td>0.062049</td>\n",
              "      <td>0.823111</td>\n",
              "      <td>0.115399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>450</td>\n",
              "      <td>0.262240</td>\n",
              "      <td>0.807123</td>\n",
              "      <td>0.467562</td>\n",
              "      <td>0.046184</td>\n",
              "      <td>0.773715</td>\n",
              "      <td>0.087165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>500</td>\n",
              "      <td>0.274906</td>\n",
              "      <td>0.804626</td>\n",
              "      <td>0.486940</td>\n",
              "      <td>0.084478</td>\n",
              "      <td>0.776533</td>\n",
              "      <td>0.152378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>550</td>\n",
              "      <td>0.287113</td>\n",
              "      <td>0.796953</td>\n",
              "      <td>0.500554</td>\n",
              "      <td>0.069442</td>\n",
              "      <td>0.759030</td>\n",
              "      <td>0.127243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>600</td>\n",
              "      <td>0.278936</td>\n",
              "      <td>0.773071</td>\n",
              "      <td>0.480268</td>\n",
              "      <td>0.056120</td>\n",
              "      <td>0.734332</td>\n",
              "      <td>0.104272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>650</td>\n",
              "      <td>0.288118</td>\n",
              "      <td>0.765863</td>\n",
              "      <td>0.494907</td>\n",
              "      <td>0.051145</td>\n",
              "      <td>0.751613</td>\n",
              "      <td>0.095773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>700</td>\n",
              "      <td>0.334216</td>\n",
              "      <td>0.788092</td>\n",
              "      <td>0.550270</td>\n",
              "      <td>0.089523</td>\n",
              "      <td>0.742416</td>\n",
              "      <td>0.159779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>750</td>\n",
              "      <td>0.298055</td>\n",
              "      <td>0.766716</td>\n",
              "      <td>0.504257</td>\n",
              "      <td>0.050133</td>\n",
              "      <td>0.744419</td>\n",
              "      <td>0.093940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>800</td>\n",
              "      <td>0.319702</td>\n",
              "      <td>0.755123</td>\n",
              "      <td>0.536700</td>\n",
              "      <td>0.077119</td>\n",
              "      <td>0.726322</td>\n",
              "      <td>0.139433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>850</td>\n",
              "      <td>0.319544</td>\n",
              "      <td>0.741956</td>\n",
              "      <td>0.531180</td>\n",
              "      <td>0.072618</td>\n",
              "      <td>0.705184</td>\n",
              "      <td>0.131677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>900</td>\n",
              "      <td>0.335903</td>\n",
              "      <td>0.775659</td>\n",
              "      <td>0.557400</td>\n",
              "      <td>0.084089</td>\n",
              "      <td>0.714159</td>\n",
              "      <td>0.150461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>950</td>\n",
              "      <td>0.328482</td>\n",
              "      <td>0.738708</td>\n",
              "      <td>0.540658</td>\n",
              "      <td>0.088039</td>\n",
              "      <td>0.713788</td>\n",
              "      <td>0.156745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.334144</td>\n",
              "      <td>0.759744</td>\n",
              "      <td>0.550131</td>\n",
              "      <td>0.088152</td>\n",
              "      <td>0.698064</td>\n",
              "      <td>0.156537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1050</td>\n",
              "      <td>0.344332</td>\n",
              "      <td>0.759436</td>\n",
              "      <td>0.560776</td>\n",
              "      <td>0.093511</td>\n",
              "      <td>0.704072</td>\n",
              "      <td>0.165096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1100</td>\n",
              "      <td>0.391950</td>\n",
              "      <td>0.793246</td>\n",
              "      <td>0.624942</td>\n",
              "      <td>0.235231</td>\n",
              "      <td>0.731514</td>\n",
              "      <td>0.355988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1150</td>\n",
              "      <td>0.358814</td>\n",
              "      <td>0.773689</td>\n",
              "      <td>0.589179</td>\n",
              "      <td>0.099056</td>\n",
              "      <td>0.727880</td>\n",
              "      <td>0.174381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1200</td>\n",
              "      <td>0.361930</td>\n",
              "      <td>0.758660</td>\n",
              "      <td>0.586275</td>\n",
              "      <td>0.133203</td>\n",
              "      <td>0.693688</td>\n",
              "      <td>0.223491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1250</td>\n",
              "      <td>0.380419</td>\n",
              "      <td>0.759841</td>\n",
              "      <td>0.615940</td>\n",
              "      <td>0.246951</td>\n",
              "      <td>0.719424</td>\n",
              "      <td>0.367689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1300</td>\n",
              "      <td>0.362470</td>\n",
              "      <td>0.774552</td>\n",
              "      <td>0.595434</td>\n",
              "      <td>0.167399</td>\n",
              "      <td>0.720463</td>\n",
              "      <td>0.271675</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-debb4e26-2995-4f27-9ec9-7f54fa5eec78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-debb4e26-2995-4f27-9ec9-7f54fa5eec78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-debb4e26-2995-4f27-9ec9-7f54fa5eec78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "!cp '/content/drive/MyDrive/'$MAIN_DRIVE_DIR'/tfidf/'$RESULTS_PATH $RESULTS_PATH\n",
        "\n",
        "\n",
        "results = read_json(RESULTS_PATH)\n",
        "results_df = pd.json_normalize(results.values())\n",
        "results_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}