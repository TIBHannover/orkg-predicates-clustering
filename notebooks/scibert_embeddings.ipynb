{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EhZi7mD3mV7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM0l0Oqp4kOn"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "TRAINING_SET_PATH = './training_set.json'\n",
        "TEST_SET_PATH = './test_set.json'\n",
        "BERT_PATH = './scibert_scivocab_uncased'\n",
        "TRAINING_REPRESENTATIONS_PATH = './scibert_training_representations.npz'\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDgUqkUQ4P2D"
      },
      "outputs": [],
      "source": [
        "## Mount Drive into Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp '/path_to_your_directory_on_google_drive/training_set.json' $TRAINING_SET_PATH\n",
        "!cp '/path_to_your_directory_on_google_drive/test_set.json' $TEST_SET_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clpY1NPE6H8x"
      },
      "outputs": [],
      "source": [
        "# Downloading the scibert model\n",
        "!wget -qO- https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar | tar --transform 's/^dbt2-0.37.50.3/dbt2/' -xv\n",
        "!tar -xzf ./scibert_scivocab_uncased/weights.tar.gz -C ./scibert_scivocab_uncased/\n",
        "!mv ./scibert_scivocab_uncased/bert_config.json ./scibert_scivocab_uncased/config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTzhzvjXB1h6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def read_json(input_path):\n",
        "    with open(input_path, encoding='utf-8') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    return json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeFajd6C4WMT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from scipy.sparse import csr_matrix\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ClusteringDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.data = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        instances = self.data['text']\n",
        "        \n",
        "        return instances.iloc[[idx]].values[0]\n",
        "\n",
        "class SciBERT:\n",
        "    DEFAULT_PATH = 'allenai/scibert_scivocab_uncased'\n",
        "\n",
        "    @staticmethod\n",
        "    def model(path=DEFAULT_PATH):\n",
        "        return BertModel.from_pretrained(path)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(path=DEFAULT_PATH):\n",
        "        return BertTokenizer.from_pretrained(path)\n",
        "\n",
        "class SciBERTVectorizer:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, tokenizer, model):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, text_batch):\n",
        "    text_tokenized = self.tokenizer(text=text_batch, padding='max_length', max_length=MAX_SEQUENCE_LENGTH, truncation=True, return_tensors='pt')\n",
        "    outputs = self.model(text_tokenized['input_ids'])\n",
        "\n",
        "    return outputs['last_hidden_state'].squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAhaNAWz5fqn"
      },
      "outputs": [],
      "source": [
        "tokenizer = SciBERT.tokenizer(BERT_PATH)\n",
        "model = SciBERT.model(BERT_PATH)\n",
        "\n",
        "vectorizer = SciBERTVectorizer().fit(tokenizer, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCQ4Ij4A6mcE"
      },
      "outputs": [],
      "source": [
        "# Import and process the training data\n",
        "import pandas as pd\n",
        "\n",
        "train_json = read_json(TRAINING_SET_PATH)\n",
        "train_df = pd.json_normalize(train_json['instances'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMv7EySDBIlV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from scipy import sparse\n",
        "\n",
        "training_data = ClusteringDataset(train_df)\n",
        "training_data_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "for i, batch in enumerate(training_data_loader):\n",
        "  print('batch {}/{}'.format(i + 1, len(training_data_loader)))\n",
        "  batch_vector_representations = vectorizer.transform(batch).detach().numpy().mean(1)\n",
        "  batch_vector_representations = sparse.csr_matrix(batch_vector_representations)\n",
        "\n",
        "  try:\n",
        "    scibert_representations = sparse.vstack((scibert_representations, batch_vector_representations))\n",
        "  except:\n",
        "    scibert_representations = batch_vector_representations\n",
        "\n",
        "  print(scibert_representations.shape)\n",
        "\n",
        "sparse.save_npz(TRAINING_REPRESENTATIONS_PATH, scibert_representations)\n",
        "!cp $TRAINING_REPRESENTATIONS_PATH '/path_to_your_directory_on_google_drive/'$TRAINING_REPRESENTATIONS_PATH "
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SciBERT_embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
